{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_deit import DeiTModel\n",
    "from md import DeiTModel as mdDeitIC\n",
    "from transformers import DeiTConfig\n",
    "import torch\n",
    "configuration = DeiTConfig()\n",
    "input = torch.rand(1, 3, 224, 224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/deit-base-distilled-patch16-224 were not used when initializing DeiTModel: ['cls_classifier.bias', 'distillation_classifier.bias', 'distillation_classifier.weight', 'cls_classifier.weight']\n",
      "- This IS expected if you are initializing DeiTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DeiTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DeiTModel were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['deit.pooler.dense.bias', 'deit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at facebook/deit-base-distilled-patch16-224 were not used when initializing DeiTModel: ['cls_classifier.bias', 'distillation_classifier.bias', 'distillation_classifier.weight', 'cls_classifier.weight']\n",
      "- This IS expected if you are initializing DeiTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DeiTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DeiTModel were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['deit.pooler.dense.bias', 'deit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_md = mdDeitIC(configuration).from_pretrained(\"facebook/deit-base-distilled-patch16-224\")\n",
    "model = DeiTModel(configuration).from_pretrained(\"facebook/deit-base-distilled-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.named_parameters at 0x7f5c811dca50>\n",
      "pooler.dense.weight\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# input = torch.rand(1, 3, 224, 224\n",
    "print(model.named_parameters())\n",
    "for named, param in model.named_parameters():\n",
    "    mp = model.get_parameter(named)\n",
    "    mpmd = model_md.get_parameter(named)\n",
    "    # if torch.equal(param, mp):\n",
    "    #     print(named)\n",
    "    # print(mp.shape, mpmd.shape)\n",
    "    # if mp.shape != mpmd.shape :\n",
    "    #     print(named)\n",
    "    if not torch.equal(mp, mpmd):\n",
    "        print(named)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********************embeddings_output**********************\n",
      "tensor([[[ 5.0128e-01,  1.6243e-02,  8.3817e-01,  ...,  7.8553e-02,\n",
      "           4.9658e-02,  1.3970e-01],\n",
      "         [ 9.2103e-01,  1.5572e-03, -1.6015e+00,  ...,  2.5535e-01,\n",
      "           6.6536e-02,  3.1074e-01],\n",
      "         [-4.6383e-02,  1.4217e-01, -1.6259e-01,  ...,  1.1278e-01,\n",
      "           1.1799e-01,  1.6359e+00],\n",
      "         ...,\n",
      "         [-2.1265e-01, -7.5257e-01, -1.1034e+00,  ...,  3.9329e-02,\n",
      "           8.2497e-02,  1.2255e+00],\n",
      "         [ 4.2688e-01, -7.4446e-01, -1.4049e+00,  ...,  7.0158e-01,\n",
      "           1.2841e+00,  1.6376e+00],\n",
      "         [ 1.0398e+00,  2.2654e-01, -1.2322e+00,  ...,  1.0717e+00,\n",
      "           1.9795e-01,  1.4843e+00]]], grad_fn=<AddBackward0>)\n",
      "*********************head_mask0**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "None\n",
      "*********************self_output**********************\n",
      "tensor([[[-1.7016e-02,  1.4758e-03, -5.9750e-03,  ...,  5.7775e-03,\n",
      "          -1.9276e-04,  1.3694e-02],\n",
      "         [-1.6890e-02,  1.3320e-03, -5.7930e-03,  ...,  6.2282e-03,\n",
      "          -8.5341e-05,  1.5133e-02],\n",
      "         [ 9.4354e-03, -1.1446e-02,  2.1878e-02,  ...,  1.2743e-02,\n",
      "          -5.8750e-03,  7.0923e-04],\n",
      "         ...,\n",
      "         [ 1.6878e-03, -1.2794e-02,  3.0233e-02,  ...,  3.5372e-03,\n",
      "           1.4779e-03, -3.0336e-04],\n",
      "         [ 2.0123e-03, -1.1739e-02,  2.8586e-02,  ...,  1.2963e-03,\n",
      "           2.1560e-04, -2.5423e-03],\n",
      "         [ 3.0769e-03, -9.9145e-03,  2.7096e-02,  ..., -7.7852e-04,\n",
      "          -3.7053e-04, -5.0736e-03]]], grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.5529, -0.0232,  0.8521,  ...,  0.0853,  0.1409, -0.2491],\n",
      "        [-0.5332, -0.0335,  0.8503,  ...,  0.1139,  0.1486, -0.1757],\n",
      "        [-0.7950,  0.0595,  0.5897,  ..., -0.1751, -0.0169, -1.0895],\n",
      "        ...,\n",
      "        [ 0.1372,  0.2131,  0.5169,  ..., -0.3964, -0.1043, -1.0803],\n",
      "        [-0.0525,  0.2422,  0.5896,  ..., -0.4555, -0.1384, -1.2216],\n",
      "        [-0.2411,  0.2096,  0.6187,  ..., -0.4869, -0.0630, -1.3106]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer0_output**********************\n",
      "tensor([[[-0.0847, -0.5479,  1.6996,  ...,  0.1478,  0.4418, -0.0526],\n",
      "         [ 0.4691, -0.8498, -0.4255,  ...,  0.5846,  0.5434,  0.2367],\n",
      "         [-1.2543,  0.0445,  0.2090,  ..., -0.4677,  0.9265, -0.7302],\n",
      "         ...,\n",
      "         [ 0.1284,  0.3958, -0.9662,  ..., -1.0026,  1.0442,  0.9242],\n",
      "         [ 0.1317, -0.0749, -1.0092,  ...,  1.3440, -0.1139,  1.3888],\n",
      "         [ 0.3651, -0.6754, -0.6348,  ...,  1.3843,  0.2334,  0.6522]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask1**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "None\n",
      "*********************self_output**********************\n",
      "tensor([[[ 0.0435, -0.0205, -0.0259,  ..., -0.0592,  0.0593,  0.0018],\n",
      "         [ 0.0301, -0.0202, -0.0331,  ..., -0.0604,  0.0707, -0.0071],\n",
      "         [ 0.0518, -0.0619,  0.0095,  ..., -0.2990,  0.1426,  0.2137],\n",
      "         ...,\n",
      "         [ 0.0267, -0.0737,  0.0068,  ..., -0.2886,  0.1365,  0.2126],\n",
      "         [ 0.0278, -0.0676, -0.0008,  ..., -0.2968,  0.1513,  0.2244],\n",
      "         [ 0.0456, -0.0637,  0.0085,  ..., -0.2969,  0.1292,  0.2262]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[ 0.1366,  0.0148, -0.0460,  ..., -0.4389, -0.0914, -0.0866],\n",
      "        [ 0.1447,  0.0078,  0.0113,  ..., -0.4844, -0.0757, -0.0778],\n",
      "        [ 0.5260, -0.5319,  0.4427,  ...,  0.3583, -0.1925, -0.0326],\n",
      "        ...,\n",
      "        [ 0.4671, -0.5894,  0.5665,  ...,  0.2956, -0.1767, -0.0369],\n",
      "        [ 0.4707, -0.6225,  0.6056,  ...,  0.2700, -0.1557, -0.0921],\n",
      "        [ 0.4221, -0.6806,  0.5998,  ...,  0.2793, -0.1182, -0.1198]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer1_output**********************\n",
      "tensor([[[ 0.1893, -0.1248,  1.4125,  ..., -0.3865,  0.1465, -0.0544],\n",
      "         [ 0.7296, -0.4626, -0.6713,  ...,  0.0077,  0.2381,  0.2694],\n",
      "         [-0.2617,  0.1757, -0.1070,  ..., -0.1736,  0.5773,  0.1128],\n",
      "         ...,\n",
      "         [ 0.9826,  0.9280, -1.1836,  ..., -0.6728,  0.3704,  1.0526],\n",
      "         [ 0.9012,  0.2175, -1.0526,  ...,  1.1369, -0.3849,  0.9102],\n",
      "         [ 1.3727, -0.6108, -0.7968,  ...,  0.6105,  0.7430,  1.1335]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask2**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "None\n",
      "*********************self_output**********************\n",
      "tensor([[[ 0.0931, -0.0638, -0.0125,  ..., -0.0128, -0.0576, -0.0162],\n",
      "         [ 0.0876, -0.0516, -0.0063,  ..., -0.0418, -0.0405, -0.0040],\n",
      "         [ 0.0848, -0.0298,  0.0074,  ..., -0.0074, -0.0512,  0.0342],\n",
      "         ...,\n",
      "         [ 0.0900, -0.0395, -0.0019,  ..., -0.0344, -0.0402,  0.0557],\n",
      "         [ 0.0875, -0.0296,  0.0051,  ..., -0.0423, -0.0312,  0.0675],\n",
      "         [ 0.0860, -0.0429,  0.0001,  ..., -0.0183, -0.0606,  0.0407]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.3113,  0.0721,  0.2445,  ..., -0.0220, -0.1588, -0.0305],\n",
      "        [-0.3275, -0.0059,  0.2949,  ..., -0.1700, -0.1094, -0.0560],\n",
      "        [-0.2775,  0.0563,  0.4831,  ..., -0.0198, -0.2265, -0.2255],\n",
      "        ...,\n",
      "        [-0.2344, -0.2587,  0.8028,  ...,  0.1175, -0.1140, -0.0526],\n",
      "        [-0.3007, -0.1056,  0.8072,  ..., -0.0228, -0.1460, -0.1360],\n",
      "        [-0.1562, -0.1317,  0.6443,  ..., -0.0173, -0.1104, -0.2087]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer2_output**********************\n",
      "tensor([[[-0.0102, -0.0615,  1.2833,  ..., -0.4345, -0.0247,  0.2525],\n",
      "         [ 0.4819, -0.5816, -0.7704,  ..., -0.1157,  0.1853,  0.5206],\n",
      "         [-0.7167,  0.6883, -0.1292,  ..., -0.7883,  0.1277,  0.2171],\n",
      "         ...,\n",
      "         [ 0.3531,  1.0953, -0.5894,  ..., -0.5990,  0.3761,  0.8282],\n",
      "         [ 0.3179,  0.0625, -0.4663,  ...,  0.9626, -0.5280,  0.4356],\n",
      "         [ 0.5067, -0.4681, -0.3942,  ...,  0.6684,  0.2698,  0.5629]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask3**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "None\n",
      "*********************self_output**********************\n",
      "tensor([[[-1.5268e-02,  1.6910e-02, -1.2532e-01,  ...,  1.6231e-01,\n",
      "          -7.5644e-03, -7.9618e-02],\n",
      "         [-1.4350e-02,  6.6376e-03, -1.0797e-01,  ...,  1.5492e-01,\n",
      "          -9.7253e-03, -9.6926e-02],\n",
      "         [-1.8071e-01,  6.3465e-02,  6.8216e-01,  ...,  1.0746e-01,\n",
      "           2.7446e-03, -9.3108e-02],\n",
      "         ...,\n",
      "         [ 2.6957e-03, -1.0297e-02, -1.2367e-01,  ..., -6.1183e-03,\n",
      "           1.3337e-02, -1.0439e-01],\n",
      "         [-2.8530e-02,  4.8519e-02, -1.4283e-01,  ..., -9.6349e-02,\n",
      "           2.5458e-02, -1.2802e-01],\n",
      "         [ 2.6204e-03,  1.8339e-04, -1.5951e-01,  ...,  6.7089e-02,\n",
      "          -1.5966e-02, -1.4217e-01]]], grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.2321,  0.2675,  0.1647,  ...,  0.3291,  0.1038, -0.5417],\n",
      "        [-0.1719,  0.2675,  0.3542,  ...,  0.3510,  0.0244, -0.5918],\n",
      "        [ 0.0605, -0.3632,  0.3567,  ...,  0.4017,  0.2079, -0.4662],\n",
      "        ...,\n",
      "        [-0.0061, -0.1253,  0.3953,  ..., -0.1902,  0.2360, -0.7405],\n",
      "        [ 0.0350, -0.1447,  0.2172,  ...,  0.2741,  0.1403, -0.7697],\n",
      "        [ 0.2627,  0.0223,  0.3221,  ..., -0.0255,  0.3433, -0.6342]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer3_output**********************\n",
      "tensor([[[-0.0464,  0.2187,  1.2966,  ..., -0.2022,  0.0876, -0.0562],\n",
      "         [ 0.5886, -0.4800, -0.8718,  ...,  0.1834,  0.2923,  0.2708],\n",
      "         [-0.7994,  0.2130, -0.5227,  ..., -0.9633,  0.1356, -0.8391],\n",
      "         ...,\n",
      "         [-0.2247,  0.1973, -0.4552,  ..., -1.3034,  0.6957,  0.7488],\n",
      "         [ 0.3810, -0.2718,  0.0904,  ...,  0.0388, -0.3957, -0.5147],\n",
      "         [ 0.8668, -0.5547, -0.2002,  ...,  0.3699,  1.2739, -0.2969]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask4**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "None\n",
      "*********************self_output**********************\n",
      "tensor([[[-1.0474e-01, -1.7472e-05,  7.3505e-02,  ..., -1.4851e-01,\n",
      "           2.3131e-01,  5.2723e-01],\n",
      "         [-1.4641e-01, -1.2145e-02,  6.8107e-02,  ..., -1.4483e-01,\n",
      "           2.1343e-01,  4.0237e-01],\n",
      "         [-1.1744e-01,  1.1475e-02,  3.2946e-02,  ..., -5.6825e-02,\n",
      "           3.0740e-01,  5.5542e-01],\n",
      "         ...,\n",
      "         [-3.2329e-01,  1.4499e-04, -1.2099e-02,  ..., -1.5112e-01,\n",
      "           2.0157e-01,  5.1951e-01],\n",
      "         [-9.9809e-02,  1.6135e-02,  7.4241e-02,  ..., -1.2555e-01,\n",
      "           2.1995e-01,  5.1434e-01],\n",
      "         [-3.2364e-01,  2.7776e-02,  2.2158e-02,  ..., -1.2334e-01,\n",
      "           2.5552e-01,  5.1943e-01]]], grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.0608,  0.0836, -0.2224,  ...,  0.3232,  0.1835, -0.7534],\n",
      "        [-0.1877, -0.0892, -0.0872,  ...,  0.4216,  0.1252, -0.8673],\n",
      "        [-0.5003, -0.1911, -0.0157,  ...,  0.3642,  0.6002, -0.5617],\n",
      "        ...,\n",
      "        [ 0.0586, -0.3092,  0.2434,  ...,  0.2079,  0.8186, -0.6136],\n",
      "        [ 0.0414, -0.3317,  0.1003,  ...,  0.3008,  0.5601, -0.9899],\n",
      "        [-0.0156, -0.0758, -0.2445,  ...,  0.2869,  0.4180, -0.8101]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer4_output**********************\n",
      "tensor([[[-0.1679,  0.4373,  0.9703,  ..., -0.0027,  0.4072, -0.7600],\n",
      "         [ 0.5684, -0.4048, -1.0970,  ...,  0.4258,  0.6161, -0.3245],\n",
      "         [-0.9122,  0.0925,  0.3150,  ..., -0.8937,  0.7490, -0.9343],\n",
      "         ...,\n",
      "         [-0.4808,  0.5215, -0.1468,  ..., -0.8479,  0.4545, -0.0066],\n",
      "         [ 0.5118,  0.0959, -0.0767,  ...,  0.0968, -0.5201, -1.9076],\n",
      "         [ 0.2627, -0.3961, -0.2410,  ...,  0.4370,  1.3697, -0.6458]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask5**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "None\n",
      "*********************self_output**********************\n",
      "tensor([[[ 0.2813,  0.0903,  0.4134,  ..., -0.0670,  0.0197, -0.0157],\n",
      "         [ 0.2266,  0.0603,  0.3779,  ...,  0.2209, -0.0571,  0.0133],\n",
      "         [ 0.1441,  0.0333,  0.2652,  ...,  0.1888,  0.0546, -0.0295],\n",
      "         ...,\n",
      "         [ 0.1219, -0.0342,  0.2049,  ...,  0.1472, -0.1011,  0.0069],\n",
      "         [ 0.1868,  0.0391,  0.2578,  ...,  0.2157, -0.1404,  0.0105],\n",
      "         [ 0.1269, -0.0304,  0.1880,  ...,  0.0535, -0.0790, -0.0055]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.2528, -0.1516,  0.3444,  ...,  0.5688, -0.0856, -0.8861],\n",
      "        [-0.2227, -0.1148,  0.2662,  ...,  0.4914, -0.0099, -0.7050],\n",
      "        [ 0.1235, -0.3302,  0.3006,  ...,  0.4549, -0.0781, -0.4670],\n",
      "        ...,\n",
      "        [ 0.4177, -0.6527,  0.1193,  ...,  0.2764,  0.0703, -0.5285],\n",
      "        [ 0.2538, -0.7445,  0.0767,  ...,  0.2471, -0.2429, -0.6494],\n",
      "        [ 0.3351, -0.5376,  0.1175,  ...,  0.4077,  0.0203, -0.5414]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer5_output**********************\n",
      "tensor([[[-0.2188,  0.3118,  1.1020,  ...,  0.3096,  0.5430, -1.1783],\n",
      "         [ 0.6058, -0.6809, -1.2587,  ...,  0.3055,  0.8101, -0.3760],\n",
      "         [ 1.2557,  0.3440,  0.7028,  ..., -0.9603,  1.6781, -0.2514],\n",
      "         ...,\n",
      "         [ 1.2017,  0.0719,  0.2124,  ..., -0.6873,  0.9849,  0.2502],\n",
      "         [ 0.9193, -0.3280, -0.7482,  ..., -0.6429,  0.4619, -2.1447],\n",
      "         [ 2.0733, -0.1351,  0.0132,  ...,  0.0036,  2.0906, -0.2428]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask6**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "None\n",
      "*********************self_output**********************\n",
      "tensor([[[-0.1602,  0.2087, -0.4860,  ...,  0.0776, -0.3074,  0.0648],\n",
      "         [-0.1211,  0.4522, -0.2439,  ..., -0.3526, -0.3411, -0.0178],\n",
      "         [-0.0566,  0.4550, -0.2273,  ...,  0.1321, -0.3509,  0.0956],\n",
      "         ...,\n",
      "         [-0.1406,  0.1415, -0.5308,  ...,  0.4339, -0.2470,  0.0384],\n",
      "         [-0.1638,  0.1498, -0.4487,  ...,  0.4391, -0.2432,  0.0518],\n",
      "         [-0.1062,  0.2970, -0.3246,  ...,  0.4467, -0.2324,  0.0592]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.0331, -0.1322,  0.0038,  ...,  0.0573,  0.3483, -0.0187],\n",
      "        [ 0.1552, -0.2036,  0.0104,  ...,  0.0538,  0.1661,  0.0914],\n",
      "        [ 0.4115, -0.1688, -0.0690,  ..., -0.1598,  0.0548,  0.1691],\n",
      "        ...,\n",
      "        [ 0.1229, -0.3935, -0.1256,  ..., -0.0197,  0.3476, -0.0520],\n",
      "        [ 0.0534, -0.4708, -0.2817,  ...,  0.0293,  0.2223, -0.0070],\n",
      "        [ 0.2722, -0.4113, -0.2562,  ..., -0.0585,  0.2202,  0.0459]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer6_output**********************\n",
      "tensor([[[-0.0191,  0.4261,  0.7044,  ...,  0.0844,  0.9668, -0.8549],\n",
      "         [ 0.5538, -0.4715, -1.6947,  ...,  0.2425,  0.6705,  0.7909],\n",
      "         [ 1.3830,  1.1683,  0.0330,  ..., -1.3539,  1.6486,  0.1659],\n",
      "         ...,\n",
      "         [ 0.6677,  0.1944, -0.4512,  ..., -1.3170,  1.1260, -0.6327],\n",
      "         [ 0.3049, -0.7230, -1.9039,  ..., -1.4837,  0.3936, -2.2814],\n",
      "         [ 1.6625,  0.4734, -0.5984,  ..., -0.7983,  2.6241, -0.1377]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask7**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "None\n",
      "*********************self_output**********************\n",
      "tensor([[[ 0.1666,  0.2068, -0.1997,  ..., -0.4435, -0.0235,  0.2010],\n",
      "         [ 0.2871,  0.1667, -0.1730,  ..., -0.4150,  0.2451,  0.4626],\n",
      "         [ 0.4211,  0.2596, -0.1806,  ..., -0.4703, -0.2171,  0.2387],\n",
      "         ...,\n",
      "         [ 0.4194,  0.0668, -0.1019,  ..., -0.4741, -0.2383,  0.1490],\n",
      "         [ 0.3626,  0.1166, -0.1498,  ..., -0.4556,  0.0148,  0.2591],\n",
      "         [ 0.3105,  0.1900, -0.1664,  ..., -0.4692, -0.0465,  0.2451]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[ 0.4867, -0.4285,  0.0520,  ...,  0.2306, -0.0236, -0.0475],\n",
      "        [ 0.7673, -0.1163,  0.1828,  ...,  0.2778, -0.6323,  0.1054],\n",
      "        [-0.0040,  0.6002,  0.0315,  ...,  0.2156, -0.8453,  0.1733],\n",
      "        ...,\n",
      "        [-0.2649,  0.4014,  0.8870,  ...,  0.1199, -0.2764, -0.0247],\n",
      "        [-0.0974,  0.4098,  0.6047,  ..., -0.0169, -0.7967,  0.0455],\n",
      "        [-0.0817,  0.4936,  0.1253,  ...,  0.2257, -0.6822,  0.0905]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer7_output**********************\n",
      "tensor([[[ 0.3709,  0.1596,  0.6217,  ...,  0.2513,  1.0011, -0.9913],\n",
      "         [ 0.6387, -0.6443, -2.2645,  ...,  0.4583, -0.5481,  0.7907],\n",
      "         [ 0.8366,  2.1571,  0.1077,  ..., -0.8635,  0.6086, -1.0046],\n",
      "         ...,\n",
      "         [ 0.1033,  0.8417, -0.1870,  ..., -1.0368,  0.5275, -1.3472],\n",
      "         [ 0.0768,  0.3283, -2.5775,  ..., -2.2203, -0.8147, -2.5965],\n",
      "         [ 0.7323,  1.3985, -0.6780,  ..., -0.2620,  0.9659, -0.7244]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask8**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "None\n",
      "*********************self_output**********************\n",
      "tensor([[[-0.0756, -0.7427,  0.1389,  ..., -0.0955,  0.0528, -0.0408],\n",
      "         [-0.2571, -0.7288,  0.1308,  ..., -0.1823,  0.0851, -0.1195],\n",
      "         [ 0.0888, -0.7538,  0.1837,  ..., -0.0062,  0.0240,  0.0303],\n",
      "         ...,\n",
      "         [-0.2014, -0.6501,  0.1628,  ...,  0.0020,  0.0337,  0.0349],\n",
      "         [-0.2446, -0.6269,  0.1211,  ..., -0.0544,  0.0395,  0.0667],\n",
      "         [ 0.0740, -0.7060,  0.1963,  ..., -0.0153,  0.0371,  0.0124]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.2635, -0.6672,  0.1536,  ..., -0.5328, -0.3629,  0.2652],\n",
      "        [-0.5466, -0.9124,  0.4041,  ..., -0.7627, -0.4221,  0.0846],\n",
      "        [-0.2379, -0.1608,  0.0415,  ..., -1.0653, -0.2555,  0.4269],\n",
      "        ...,\n",
      "        [ 0.0666, -0.3392,  0.1975,  ..., -0.6421,  0.2106,  0.2815],\n",
      "        [-0.0560, -0.2731,  0.1975,  ..., -0.8614, -0.1668,  0.3017],\n",
      "        [-0.1825, -0.2193,  0.1429,  ..., -1.0689,  0.0885,  0.3134]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer8_output**********************\n",
      "tensor([[[ 0.0527, -0.5387,  0.6522,  ..., -0.0535,  0.5519, -1.0212],\n",
      "         [-0.1847, -0.6970, -2.5463,  ..., -0.0059, -1.5785,  0.7575],\n",
      "         [ 0.8186,  2.0693, -0.3704,  ..., -2.2663,  0.1405, -0.4238],\n",
      "         ...,\n",
      "         [ 0.2953,  0.2922, -0.4084,  ..., -2.3347,  0.4003, -1.0746],\n",
      "         [-0.2398, -0.0306, -2.7212,  ..., -3.8395, -1.3001, -2.4428],\n",
      "         [ 0.1104,  1.1409, -1.0424,  ..., -1.3500,  0.5774, -0.6713]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask9**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "None\n",
      "*********************self_output**********************\n",
      "tensor([[[-0.0724, -0.0365,  0.0164,  ..., -0.1850,  0.2411,  0.1648],\n",
      "         [-0.2289, -0.0089,  0.1125,  ..., -0.1538,  0.2610,  0.2063],\n",
      "         [-0.0617,  0.0656, -0.3186,  ..., -0.2160,  0.2090,  0.2337],\n",
      "         ...,\n",
      "         [-0.0515,  0.0684, -0.3392,  ..., -0.3211,  0.2944,  0.2886],\n",
      "         [-0.0470,  0.0735, -0.3578,  ..., -0.1960,  0.1783,  0.2220],\n",
      "         [-0.0508,  0.1009, -0.4453,  ..., -0.3142,  0.2780,  0.3024]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.2564,  0.0049,  0.3105,  ...,  0.5345,  0.1629,  0.2218],\n",
      "        [-0.2423, -0.0952,  0.1533,  ...,  0.6890,  0.0860,  0.0847],\n",
      "        [-0.2313,  0.3584, -0.0992,  ..., -0.2214,  0.5752,  0.0901],\n",
      "        ...,\n",
      "        [ 0.1780,  0.1435, -0.1722,  ...,  0.1084,  0.8486,  0.1546],\n",
      "        [ 0.0730,  0.2582, -0.3295,  ..., -0.1055,  0.7553,  0.1861],\n",
      "        [ 0.0275,  0.1566, -0.0038,  ...,  0.0920,  0.7098,  0.2049]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer9_output**********************\n",
      "tensor([[[-0.4225, -0.5942,  1.0120,  ...,  0.4851,  1.0462, -1.3085],\n",
      "         [-0.1775, -1.1577, -3.5660,  ...,  1.0245, -1.3518, -0.0621],\n",
      "         [ 0.3010,  2.0994, -0.9012,  ..., -2.2351,  0.6312, -0.3639],\n",
      "         ...,\n",
      "         [ 0.0192,  0.2678, -1.0899,  ..., -1.8296,  1.2195, -1.5524],\n",
      "         [-0.1047, -0.0662, -3.1222,  ..., -3.6186, -0.7079, -2.4561],\n",
      "         [-0.3594,  1.4041, -1.6473,  ..., -0.8628,  1.4315, -0.5738]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask10**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "None\n",
      "*********************self_output**********************\n",
      "tensor([[[ 0.0457, -0.2855,  0.3792,  ...,  0.0862,  0.2079,  0.0163],\n",
      "         [ 0.1625, -0.2633,  0.4709,  ...,  0.0267,  0.3038, -0.1890],\n",
      "         [ 0.1575, -0.3680,  0.4742,  ..., -0.1110,  0.3026, -0.2332],\n",
      "         ...,\n",
      "         [ 0.0746, -0.2776,  0.3586,  ..., -0.1118,  0.2825, -0.1810],\n",
      "         [ 0.1932, -0.2550,  0.4555,  ..., -0.0896,  0.2814, -0.1956],\n",
      "         [ 0.0603, -0.2946,  0.3594,  ..., -0.0752,  0.2854, -0.1714]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.5038, -0.2938,  0.3383,  ...,  0.4126,  0.7277,  0.4535],\n",
      "        [-0.7437, -0.4318,  0.1214,  ...,  0.4951,  0.4742,  0.2762],\n",
      "        [-0.6120, -0.4845,  0.0911,  ...,  0.0604,  0.6439,  0.3062],\n",
      "        ...,\n",
      "        [-0.3778, -0.3456, -0.0342,  ...,  0.1809,  0.3060,  0.2250],\n",
      "        [-0.6428, -0.3593, -0.0118,  ...,  0.2530,  0.3865,  0.2866],\n",
      "        [-0.4913, -0.4530,  0.1332,  ...,  0.0713,  0.5300,  0.3432]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer10_output**********************\n",
      "tensor([[[-0.9397, -0.5991,  1.5510,  ...,  0.3901,  2.0173, -1.1865],\n",
      "         [-1.4179, -0.2366, -3.3585,  ...,  0.7713, -1.6226, -0.4403],\n",
      "         [-0.0375,  1.1625, -0.4594,  ..., -1.8757,  1.6335,  0.2921],\n",
      "         ...,\n",
      "         [-0.3615,  0.7940, -1.2023,  ..., -1.7449,  1.5663, -1.0357],\n",
      "         [-0.5742,  0.0659, -2.7543,  ..., -3.2707, -0.1197, -1.9678],\n",
      "         [-0.3474,  0.6485, -1.2960,  ..., -0.3745,  2.2946,  0.2256]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask11**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "None\n",
      "*********************self_output**********************\n",
      "tensor([[[-0.0896, -0.0164,  0.2924,  ..., -0.0853,  0.1532, -0.7941],\n",
      "         [-0.0826, -0.1487,  0.1738,  ...,  0.1206,  0.3462, -0.6574],\n",
      "         [-0.1226, -0.1781,  0.0058,  ...,  0.0172,  0.3936, -0.8030],\n",
      "         ...,\n",
      "         [-0.1198, -0.1820, -0.0881,  ..., -0.0778,  0.4154, -0.7825],\n",
      "         [-0.1385, -0.1759,  0.0404,  ..., -0.0278,  0.4087, -0.8782],\n",
      "         [-0.1373, -0.1568,  0.0663,  ..., -0.0279,  0.3724, -0.7851]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-2.1687e-01, -1.4198e-01,  4.4801e-01,  ...,  3.5376e-01,\n",
      "         -9.3825e-01, -1.3260e-01],\n",
      "        [-3.8356e-01, -8.5005e-02,  1.6080e-01,  ...,  4.7114e-01,\n",
      "         -9.1087e-01,  6.8548e-03],\n",
      "        [-4.5674e-01, -2.5779e-01, -7.1612e-04,  ...,  4.2458e-01,\n",
      "         -7.7726e-01, -2.9994e-02],\n",
      "        ...,\n",
      "        [-2.8161e-01, -2.1837e-01, -1.7489e-02,  ...,  4.7883e-01,\n",
      "         -7.3982e-01,  5.6231e-02],\n",
      "        [-3.6349e-01, -3.1984e-01,  1.6365e-03,  ...,  3.8150e-01,\n",
      "         -7.2480e-01, -3.4713e-02],\n",
      "        [-3.6268e-01, -2.0337e-01,  9.0331e-03,  ...,  4.5211e-01,\n",
      "         -7.5787e-01, -2.2370e-02]], grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer11_output**********************\n",
      "tensor([[[-1.4426, -0.2991,  0.6324,  ...,  0.4986,  0.4502, -1.0270],\n",
      "         [-1.6176, -0.4553, -2.8726,  ...,  0.9693, -2.6113, -0.4838],\n",
      "         [-0.4482,  1.4497, -0.6940,  ..., -1.4244,  0.8686,  0.6693],\n",
      "         ...,\n",
      "         [-0.9385,  0.3047, -1.2803,  ..., -1.2311,  0.8233, -0.8675],\n",
      "         [-1.0456, -0.2520, -2.9453,  ..., -2.9485, -0.5258, -1.6441],\n",
      "         [-0.8086,  0.7141, -1.6020,  ...,  0.0918,  1.5128,  0.4022]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************encoder_output**********************\n",
      "BaseModelOutput(last_hidden_state=tensor([[[-1.4426, -0.2991,  0.6324,  ...,  0.4986,  0.4502, -1.0270],\n",
      "         [-1.6176, -0.4553, -2.8726,  ...,  0.9693, -2.6113, -0.4838],\n",
      "         [-0.4482,  1.4497, -0.6940,  ..., -1.4244,  0.8686,  0.6693],\n",
      "         ...,\n",
      "         [-0.9385,  0.3047, -1.2803,  ..., -1.2311,  0.8233, -0.8675],\n",
      "         [-1.0456, -0.2520, -2.9453,  ..., -2.9485, -0.5258, -1.6441],\n",
      "         [-0.8086,  0.7141, -1.6020,  ...,  0.0918,  1.5128,  0.4022]]],\n",
      "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)\n",
      "BaseModelOutputWithPooling(last_hidden_state=tensor([[[-0.6268, -0.1452,  0.1737,  ...,  0.0554,  0.2192, -0.4375],\n",
      "         [-0.8256, -0.2249, -1.5093,  ...,  0.3412, -1.2513, -0.2334],\n",
      "         [-0.1689,  0.5180, -0.3653,  ..., -0.7537,  0.4217,  0.3187],\n",
      "         ...,\n",
      "         [-0.4656,  0.1011, -0.7110,  ..., -0.7642,  0.4427, -0.4230],\n",
      "         [-0.5303, -0.1491, -1.5225,  ..., -1.6171, -0.2300, -0.8180],\n",
      "         [-0.3811,  0.2535, -0.8306,  ..., -0.1107,  0.7329,  0.1986]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-5.2533e-01, -9.9397e-02,  2.7675e-01, -7.0905e-02,  2.0569e-01,\n",
      "          3.1633e-01, -6.9171e-02,  7.6873e-02,  1.2065e-01, -3.7137e-01,\n",
      "          5.0313e-02, -7.6017e-02,  2.3560e-02,  4.8335e-01,  1.5068e-01,\n",
      "         -3.3426e-01, -6.0520e-01, -6.1327e-02,  8.9775e-03, -2.6344e-01,\n",
      "          1.2127e-01, -3.8587e-01, -3.2581e-01, -5.8482e-01,  1.3476e-01,\n",
      "         -9.5546e-02,  7.0337e-02, -1.9240e-03, -3.8738e-01, -3.8851e-01,\n",
      "         -2.0593e-01,  3.6539e-01,  2.6707e-01, -6.3852e-01, -1.1616e-01,\n",
      "          5.4528e-01,  2.3522e-01, -2.7854e-01, -2.1803e-01, -4.2473e-02,\n",
      "          3.3842e-05,  3.1951e-01,  3.3930e-02, -4.2983e-01,  3.1668e-01,\n",
      "         -1.5559e-01,  9.1973e-02,  6.7508e-01, -1.2820e-01,  1.4689e-01,\n",
      "          1.8742e-01, -1.9911e-01,  5.0891e-01,  1.4973e-01,  6.7908e-02,\n",
      "          2.4344e-01,  2.4493e-01, -7.2647e-02, -4.7696e-01,  1.6333e-01,\n",
      "          2.3804e-01, -1.0373e-01,  1.8767e-01, -1.1803e-01, -1.8023e-01,\n",
      "          2.1523e-01,  1.7602e-01,  2.2923e-01,  3.4513e-01,  3.9916e-01,\n",
      "          3.7637e-01,  4.2215e-01, -1.0614e-01, -3.3722e-01,  2.3792e-02,\n",
      "          1.4782e-03,  2.2695e-01,  1.6576e-01, -4.2128e-01, -3.3970e-02,\n",
      "         -3.4805e-01,  8.7626e-02, -1.8991e-01,  3.5347e-02,  2.9282e-01,\n",
      "          4.1177e-01, -2.4656e-01,  2.2695e-01,  5.1578e-02,  2.5101e-01,\n",
      "          1.6244e-01, -1.0344e-01, -1.1291e-01, -2.3547e-01, -1.7260e-01,\n",
      "          1.5508e-01, -2.5014e-02, -2.0760e-01,  4.4743e-01,  1.9661e-01,\n",
      "         -4.3800e-01,  1.1551e-01, -2.4146e-02, -2.8162e-01, -7.0955e-02,\n",
      "          3.3785e-01,  1.8483e-01,  3.1170e-04,  1.5922e-01, -3.9556e-01,\n",
      "         -8.2954e-02,  4.6919e-01, -4.3941e-01,  2.5461e-01,  1.2426e-01,\n",
      "         -4.3869e-01,  2.2698e-01, -1.3347e-01,  3.6545e-01, -5.5109e-01,\n",
      "          8.0895e-02,  4.6085e-01,  1.4657e-01,  8.6054e-02, -1.4266e-01,\n",
      "          1.0225e-01, -2.5087e-02, -1.7225e-01, -3.4027e-01, -5.4752e-01,\n",
      "          1.0504e-01,  1.8919e-01,  1.1458e-01,  3.6267e-01, -3.8644e-02,\n",
      "          1.3399e-01,  3.8137e-02, -1.2612e-01, -6.8844e-02,  2.5977e-01,\n",
      "         -1.0497e-02, -2.0228e-02,  9.0179e-05, -3.1040e-01, -5.6532e-03,\n",
      "         -1.1215e-01,  2.7627e-01,  2.2147e-01, -1.3167e-01, -9.3486e-02,\n",
      "         -1.4915e-01, -3.8691e-01,  3.9960e-02,  2.5056e-01, -1.6726e-02,\n",
      "         -1.0082e-02,  4.1952e-01, -2.1157e-01, -5.5586e-01, -1.0093e-01,\n",
      "         -2.3763e-01,  7.1931e-01,  1.9309e-01, -2.7401e-01,  3.2957e-01,\n",
      "         -1.7106e-01, -7.3062e-02, -1.9328e-01,  2.4719e-01,  1.0819e-02,\n",
      "         -3.1426e-01, -1.4262e-01, -2.4780e-01,  2.9672e-01,  2.0699e-01,\n",
      "         -3.4409e-01,  1.2199e-01,  3.6499e-01, -8.9713e-02,  2.5123e-01,\n",
      "          2.4614e-01,  1.6252e-02, -4.1387e-02,  3.0687e-03,  3.5458e-01,\n",
      "         -1.4104e-01, -1.3065e-01, -5.3262e-01,  5.4983e-01,  1.3917e-01,\n",
      "         -2.1888e-01, -3.4383e-02, -4.0586e-01,  1.3802e-01, -1.4710e-01,\n",
      "         -5.2386e-01,  3.3494e-01,  3.5493e-01, -5.0498e-01, -3.3822e-01,\n",
      "          3.4617e-01, -5.9272e-01, -1.0687e-01,  4.7144e-02,  2.5857e-01,\n",
      "          5.4945e-02, -7.8346e-02, -6.1007e-01,  1.4715e-02,  2.7398e-01,\n",
      "         -1.2486e-01, -3.8013e-01, -4.6462e-01, -1.0454e-02,  2.2615e-01,\n",
      "          2.0215e-01,  3.5328e-01,  2.3347e-01,  3.2345e-01, -6.7199e-01,\n",
      "         -1.0957e-01, -4.5568e-01, -1.1527e-02,  1.0298e-01, -1.6896e-01,\n",
      "         -1.5887e-02, -3.4095e-01,  2.6963e-01,  3.7917e-01, -5.0782e-01,\n",
      "         -5.7429e-02, -2.2691e-01, -3.8198e-01,  4.0132e-02, -3.1992e-01,\n",
      "         -1.5311e-01,  1.8397e-01, -3.0135e-01, -2.9556e-01,  6.2850e-02,\n",
      "          4.7073e-02, -4.8776e-01,  4.4691e-02,  4.7308e-02,  1.5397e-01,\n",
      "         -3.1745e-01, -7.3783e-02, -1.3480e-01, -1.1327e-01, -6.5526e-02,\n",
      "         -1.9837e-02,  3.4184e-01,  4.0345e-02,  2.0938e-01,  2.5152e-01,\n",
      "         -1.8803e-02, -3.7335e-01, -2.7929e-01,  1.6005e-01,  3.6780e-02,\n",
      "         -2.6609e-01,  2.4792e-01, -1.2667e-01, -4.6834e-01, -2.6609e-01,\n",
      "          4.1224e-02, -1.7170e-01,  2.2485e-01,  2.9858e-01,  1.8820e-01,\n",
      "         -8.4872e-02,  1.7949e-01,  1.7370e-01, -1.7039e-03, -3.5542e-02,\n",
      "          2.3311e-01,  2.0707e-01,  3.2186e-01,  7.3398e-02,  4.3185e-01,\n",
      "         -3.2243e-01,  2.1436e-01,  3.4877e-01, -3.7926e-01, -3.1594e-02,\n",
      "         -7.1035e-01,  6.0467e-01, -3.1681e-01,  2.2132e-01, -1.2306e-01,\n",
      "          9.5151e-02, -5.3114e-01,  2.8872e-02,  5.2650e-02,  3.8614e-01,\n",
      "         -2.7810e-01, -1.6996e-01, -3.9311e-01, -8.1915e-02,  9.3361e-02,\n",
      "          3.4433e-02, -5.5964e-01, -4.2914e-02,  3.5817e-02, -2.0049e-01,\n",
      "         -1.4700e-01, -1.1596e-01, -1.5059e-01,  7.4512e-01,  3.8790e-01,\n",
      "         -5.7504e-02, -3.8344e-01,  9.2151e-02,  6.6585e-02,  3.8705e-01,\n",
      "         -7.8639e-02,  2.8637e-01, -2.6516e-01,  7.7257e-02, -1.6336e-01,\n",
      "          1.2047e-01,  1.6969e-01, -3.4904e-02, -1.5076e-01, -2.6229e-01,\n",
      "         -5.5971e-01, -7.7008e-02,  5.1320e-02,  2.8826e-01, -3.0734e-01,\n",
      "          3.0190e-01,  6.1358e-02,  4.3576e-01, -2.4260e-01, -4.0154e-01,\n",
      "          5.5972e-01, -5.0474e-01,  3.4695e-01, -2.2019e-01,  5.0635e-01,\n",
      "         -1.2295e-01, -2.2325e-01,  4.9124e-01, -1.6179e-01,  1.4313e-01,\n",
      "         -1.2636e-01, -3.5862e-01, -2.9775e-01,  1.4728e-01,  1.9046e-01,\n",
      "          4.9499e-02, -6.2263e-01, -3.1400e-01,  1.1370e-01, -8.2464e-02,\n",
      "         -5.1037e-01, -2.7315e-01,  1.0788e-01,  4.2043e-01,  2.2332e-02,\n",
      "         -1.6560e-01, -3.3429e-01,  1.8492e-01,  2.0621e-01, -1.0736e-01,\n",
      "          5.8208e-02,  3.2068e-01, -8.1317e-02,  3.6149e-01,  1.4829e-01,\n",
      "         -1.2286e-01,  2.0269e-01,  2.2875e-01, -4.1797e-01,  2.8483e-01,\n",
      "         -1.4340e-01, -1.9792e-01,  1.8703e-01,  1.0834e-01, -4.9798e-02,\n",
      "          4.0977e-01, -1.4625e-01, -1.7716e-01, -2.5943e-01, -4.9708e-01,\n",
      "          1.4067e-01,  7.7423e-02,  2.1300e-01, -1.7196e-01, -2.4571e-01,\n",
      "          6.5707e-01,  5.4006e-01,  1.5424e-01,  2.9860e-01, -3.6321e-01,\n",
      "          2.7376e-01, -4.9398e-01,  4.8160e-01, -1.2807e-01, -1.6810e-01,\n",
      "          2.6261e-01, -4.3757e-02, -1.6179e-01,  2.3465e-02, -1.6720e-01,\n",
      "          2.5972e-01, -4.2885e-01,  4.1577e-01,  1.0427e-01,  2.7193e-01,\n",
      "          3.7134e-02,  5.2887e-01, -1.3954e-01, -3.1630e-01, -2.3272e-03,\n",
      "         -1.3901e-01, -2.2917e-01,  3.8020e-03, -5.3896e-02,  1.9172e-01,\n",
      "          1.6876e-01,  3.3792e-02,  3.2293e-01,  7.0036e-02, -1.1893e-01,\n",
      "         -4.8719e-01, -4.2348e-01,  3.1334e-01,  4.5788e-01,  4.3068e-01,\n",
      "         -2.4787e-01,  1.9387e-01,  1.2421e-01, -1.0913e-01,  3.5335e-01,\n",
      "         -8.5047e-02, -2.0575e-01, -6.2806e-01, -4.6313e-01, -1.5762e-02,\n",
      "          6.5384e-01,  3.9486e-01,  4.3468e-02, -3.0057e-02, -3.9103e-01,\n",
      "          8.7299e-02, -7.5519e-02, -1.8926e-01, -1.2001e-01,  2.1685e-01,\n",
      "         -1.3195e-01, -3.5355e-01, -2.9930e-01, -4.9688e-03,  4.4398e-02,\n",
      "          1.2730e-01, -4.1814e-01,  2.2001e-01, -3.5166e-01,  2.4315e-01,\n",
      "          5.3978e-02, -1.5134e-01, -1.2381e-01, -2.5937e-01,  4.7838e-01,\n",
      "         -1.5584e-01,  1.7092e-01, -4.8816e-01, -7.8423e-02,  4.7885e-01,\n",
      "         -2.7119e-01, -4.0860e-01, -1.1016e-01,  1.1796e-01,  2.7361e-01,\n",
      "          2.7914e-01, -4.8752e-01,  4.1430e-01,  3.2542e-01, -2.1002e-01,\n",
      "         -6.4540e-02, -6.5006e-02, -2.0268e-01, -1.0782e-01, -5.1329e-02,\n",
      "          3.2570e-01,  1.7933e-02, -1.3828e-01,  2.4875e-01,  4.8895e-01,\n",
      "         -3.8632e-01, -7.3101e-02, -1.4651e-01, -1.2819e-01,  1.8774e-01,\n",
      "          5.6081e-02,  6.1503e-02,  1.1848e-01,  4.6782e-01, -1.4127e-01,\n",
      "          2.9010e-02, -4.4557e-01, -2.7914e-01,  2.1863e-02, -2.5047e-01,\n",
      "          3.1860e-01, -2.1412e-01, -1.6415e-01,  2.9077e-01,  1.0040e-02,\n",
      "         -4.6402e-01, -1.4150e-01, -2.2101e-01, -1.2934e-01,  3.4082e-01,\n",
      "         -3.4479e-01, -1.7503e-01,  3.5910e-01,  1.1581e-01, -4.0505e-01,\n",
      "          1.5307e-02, -8.1453e-02, -2.7790e-01,  2.7191e-02,  5.4769e-01,\n",
      "          2.6675e-01, -4.6168e-01,  2.3725e-01, -1.0532e-01, -2.8357e-01,\n",
      "         -1.5954e-01,  2.1235e-01,  2.4800e-02, -1.4333e-01,  7.8826e-02,\n",
      "          4.7295e-01, -5.1105e-01,  1.3324e-01, -1.5011e-01,  7.6892e-02,\n",
      "         -6.8778e-02, -6.3732e-03, -1.4624e-01, -7.8180e-02, -4.8482e-01,\n",
      "          8.2756e-02, -1.3346e-02,  1.3938e-01, -6.5713e-01,  1.9876e-01,\n",
      "          3.6556e-01, -5.7405e-01,  4.5599e-01,  3.8593e-02,  4.9939e-01,\n",
      "         -3.0385e-01, -6.4873e-01, -1.3707e-01, -4.0714e-01, -1.0148e-01,\n",
      "         -1.3192e-01, -2.7979e-01,  4.1422e-01,  3.0284e-01, -1.0991e-01,\n",
      "         -1.8372e-01, -4.2386e-02, -2.1380e-01, -3.8031e-01, -5.1935e-02,\n",
      "          3.2755e-01, -1.0210e-01,  3.7017e-01,  2.0337e-01,  1.5265e-01,\n",
      "          2.7305e-02, -5.3003e-01,  2.4865e-01,  1.9884e-01, -2.8462e-01,\n",
      "         -1.1940e-01,  1.9741e-01,  7.1633e-03, -1.0487e-01, -3.8456e-01,\n",
      "          2.8399e-01,  3.9287e-01,  3.5886e-01, -6.1373e-02,  1.5783e-01,\n",
      "          4.4162e-01,  1.8894e-01,  5.1896e-02,  1.2196e-01, -3.6404e-01,\n",
      "          5.1100e-01, -4.0571e-01,  1.4165e-01,  4.0925e-01,  1.1140e-01,\n",
      "          2.5279e-01, -2.0830e-01, -4.4718e-01,  1.3442e-01, -1.8826e-01,\n",
      "         -1.2787e-01,  1.2923e-01, -1.2678e-01, -6.3528e-01,  1.1374e-01,\n",
      "          3.2078e-01, -4.6638e-01,  7.4013e-02,  4.4334e-01,  1.7854e-01,\n",
      "          4.4583e-03,  1.3534e-01,  2.5959e-01, -5.1568e-02,  4.9888e-01,\n",
      "          4.0104e-02, -1.0983e-01, -5.8961e-02, -4.4731e-01,  4.1753e-02,\n",
      "          1.2760e-01,  1.6549e-02,  1.1957e-01, -2.1892e-01,  2.0904e-02,\n",
      "         -8.7563e-02,  3.0619e-01, -2.9375e-01,  1.9310e-01, -2.9966e-01,\n",
      "          2.6740e-01, -3.1781e-01, -2.7235e-01,  2.0504e-01, -2.2616e-01,\n",
      "         -4.4127e-02,  3.4711e-01,  2.7785e-01,  9.9793e-02, -1.9654e-01,\n",
      "         -2.9619e-02,  7.7894e-02, -2.2436e-02, -1.5334e-01, -6.3778e-01,\n",
      "         -2.4560e-02, -1.7736e-01, -4.0660e-01,  2.2198e-01, -4.8949e-01,\n",
      "          4.6011e-01, -1.0813e-01,  3.7934e-01,  1.8375e-01,  2.6619e-01,\n",
      "          6.4882e-02,  1.7633e-01, -2.8245e-01, -7.1231e-01,  1.2602e-01,\n",
      "          4.1434e-01,  1.4278e-01, -5.2933e-02, -3.8092e-01, -2.7486e-02,\n",
      "          1.0988e-01,  1.0172e-01, -4.2420e-01, -5.1188e-02,  5.4340e-01,\n",
      "         -4.2102e-01, -5.3998e-01,  9.8213e-02, -1.3440e-01, -1.0057e-01,\n",
      "         -3.1566e-01, -2.5376e-01,  6.2975e-01, -2.6039e-01,  2.8759e-01,\n",
      "         -1.0745e-01, -4.8169e-01, -1.0289e-04,  6.1404e-01,  4.6745e-03,\n",
      "         -3.1951e-01,  2.2662e-01, -2.9957e-01,  2.7677e-01, -6.7567e-01,\n",
      "         -1.7849e-01, -4.4976e-02, -2.3825e-01, -2.5937e-01, -2.0232e-01,\n",
      "         -5.0223e-02,  8.0731e-02, -2.3228e-01,  1.1576e-01, -1.5758e-01,\n",
      "         -1.9097e-01, -4.4814e-02, -1.3028e-02,  1.7706e-01,  4.7214e-01,\n",
      "         -2.0748e-01,  7.4166e-02,  5.1824e-01,  2.7563e-01,  4.0605e-01,\n",
      "          2.0266e-01,  8.1552e-02, -2.8348e-01,  1.9735e-01,  1.1398e-01,\n",
      "         -5.1380e-01,  2.2426e-01,  3.6974e-01,  2.2692e-01,  3.2408e-01,\n",
      "          1.6738e-01, -1.1887e-01, -1.8469e-02, -2.5040e-01,  2.3268e-01,\n",
      "         -1.6284e-01, -3.1867e-01,  2.5089e-02, -4.6265e-01, -5.3460e-01,\n",
      "         -1.8466e-01, -3.9853e-02, -4.0072e-02, -1.7367e-01,  6.6701e-01,\n",
      "         -1.1825e-01,  1.7950e-01, -3.1271e-01, -2.4371e-01,  2.9756e-01,\n",
      "         -2.0006e-01,  5.5803e-01,  1.9072e-02, -3.5699e-01, -1.0522e-01,\n",
      "          4.3014e-01,  2.0903e-01, -8.0823e-02, -2.0162e-01,  1.1586e-01,\n",
      "          3.2209e-01, -4.0351e-01,  2.2581e-03, -6.2125e-02,  3.1831e-01,\n",
      "          3.6413e-01, -5.2975e-02,  1.4769e-01,  1.4379e-01,  6.0853e-01,\n",
      "          6.0360e-02, -2.2283e-01, -1.7519e-01]], grad_fn=<TanhBackward0>), hidden_states=None, attentions=None)\n",
      "end\n",
      "False False True\n",
      "*********************embeddings_output**********************\n",
      "tensor([[[ 5.0128e-01,  1.6243e-02,  8.3817e-01,  ...,  7.8553e-02,\n",
      "           4.9658e-02,  1.3970e-01],\n",
      "         [ 9.2103e-01,  1.5572e-03, -1.6015e+00,  ...,  2.5535e-01,\n",
      "           6.6536e-02,  3.1074e-01],\n",
      "         [-4.6383e-02,  1.4217e-01, -1.6259e-01,  ...,  1.1278e-01,\n",
      "           1.1799e-01,  1.6359e+00],\n",
      "         ...,\n",
      "         [-2.1265e-01, -7.5257e-01, -1.1034e+00,  ...,  3.9329e-02,\n",
      "           8.2497e-02,  1.2255e+00],\n",
      "         [ 4.2688e-01, -7.4446e-01, -1.4049e+00,  ...,  7.0158e-01,\n",
      "           1.2841e+00,  1.6376e+00],\n",
      "         [ 1.0398e+00,  2.2654e-01, -1.2322e+00,  ...,  1.0717e+00,\n",
      "           1.9795e-01,  1.4843e+00]]], grad_fn=<AddBackward0>)\n",
      "*********************head_mask0**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "False\n",
      "*********************self_output**********************\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.5088, -0.0549,  0.1718,  ...,  0.0082,  0.0601, -0.0654],\n",
      "        [-0.5088, -0.0549,  0.1718,  ...,  0.0082,  0.0601, -0.0654],\n",
      "        [-0.5088, -0.0549,  0.1718,  ...,  0.0082,  0.0601, -0.0654],\n",
      "        ...,\n",
      "        [-0.5088, -0.0549,  0.1718,  ...,  0.0082,  0.0601, -0.0654],\n",
      "        [-0.5088, -0.0549,  0.1718,  ...,  0.0082,  0.0601, -0.0654],\n",
      "        [-0.5088, -0.0549,  0.1718,  ...,  0.0082,  0.0601, -0.0654]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer0_output**********************\n",
      "tensor([[[ 0.0550, -0.5153,  0.9964,  ..., -0.0603,  0.3945,  0.1438],\n",
      "         [ 0.5701, -0.8682, -1.0643,  ...,  0.4390,  0.4469,  0.3543],\n",
      "         [-0.3059, -0.1133,  0.0231,  ..., -0.3561,  0.4689,  0.4794],\n",
      "         ...,\n",
      "         [-0.6510, -0.4641, -1.1060,  ..., -0.6872,  0.4325,  1.1443],\n",
      "         [-0.4945, -0.7380, -1.3868,  ...,  0.9848,  0.4292,  1.7898],\n",
      "         [-0.0516, -0.4434, -1.1370,  ...,  1.4347, -0.0391,  1.1632]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask1**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "False\n",
      "*********************self_output**********************\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.0608,  0.1026,  0.0447,  ..., -0.0643, -0.2938,  0.0014],\n",
      "        [-0.0608,  0.1026,  0.0447,  ..., -0.0643, -0.2938,  0.0014],\n",
      "        [-0.0608,  0.1026,  0.0447,  ..., -0.0643, -0.2938,  0.0014],\n",
      "        ...,\n",
      "        [-0.0608,  0.1026,  0.0447,  ..., -0.0643, -0.2938,  0.0014],\n",
      "        [-0.0608,  0.1026,  0.0447,  ..., -0.0643, -0.2938,  0.0014],\n",
      "        [-0.0608,  0.1026,  0.0447,  ..., -0.0643, -0.2938,  0.0014]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer1_output**********************\n",
      "tensor([[[ 0.0668,  0.0363,  0.8696,  ..., -0.2003, -0.1129,  0.2289],\n",
      "         [ 0.5447, -0.3763, -1.2119,  ...,  0.2961, -0.0704,  0.4536],\n",
      "         [-0.4598,  0.5391, -0.1910,  ..., -0.3368,  0.0856,  0.3932],\n",
      "         ...,\n",
      "         [-0.1761,  0.4384, -1.3784,  ..., -0.8341,  0.0208,  0.8922],\n",
      "         [-0.2282,  0.0527, -1.5455,  ...,  0.8972,  0.1003,  1.6247],\n",
      "         [ 0.1007,  0.2214, -1.2893,  ...,  1.3305, -0.3741,  0.9899]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask2**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "False\n",
      "*********************self_output**********************\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.2372,  0.1509,  0.0667,  ..., -0.1031, -0.3339, -0.0107],\n",
      "        [-0.2372,  0.1509,  0.0667,  ..., -0.1031, -0.3339, -0.0107],\n",
      "        [-0.2372,  0.1509,  0.0667,  ..., -0.1031, -0.3339, -0.0107],\n",
      "        ...,\n",
      "        [-0.2372,  0.1509,  0.0667,  ..., -0.1031, -0.3339, -0.0107],\n",
      "        [-0.2372,  0.1509,  0.0667,  ..., -0.1031, -0.3339, -0.0107],\n",
      "        [-0.2372,  0.1509,  0.0667,  ..., -0.1031, -0.3339, -0.0107]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer2_output**********************\n",
      "tensor([[[-0.2510,  0.1437,  0.6447,  ..., -0.3547, -0.4254,  0.3269],\n",
      "         [ 0.1610, -0.2105, -1.4365,  ...,  0.1791, -0.3084,  0.5195],\n",
      "         [-0.7123,  0.4658, -0.2802,  ..., -0.3749,  0.0288, -0.2469],\n",
      "         ...,\n",
      "         [-0.6963,  0.3832, -1.2144,  ..., -0.8839, -0.3137,  0.5076],\n",
      "         [-0.7461, -0.2040, -1.2566,  ...,  1.0408, -0.0232,  1.1242],\n",
      "         [-0.0665, -0.1607, -1.0058,  ...,  0.7116, -0.3807,  0.4959]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask3**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "False\n",
      "*********************self_output**********************\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.2899,  0.3126, -0.0396,  ...,  0.1342, -0.0085, -0.3411],\n",
      "        [-0.2899,  0.3126, -0.0396,  ...,  0.1342, -0.0085, -0.3411],\n",
      "        [-0.2899,  0.3126, -0.0396,  ...,  0.1342, -0.0085, -0.3411],\n",
      "        ...,\n",
      "        [-0.2899,  0.3126, -0.0396,  ...,  0.1342, -0.0085, -0.3411],\n",
      "        [-0.2899,  0.3126, -0.0396,  ...,  0.1342, -0.0085, -0.3411],\n",
      "        [-0.2899,  0.3126, -0.0396,  ...,  0.1342, -0.0085, -0.3411]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer3_output**********************\n",
      "tensor([[[-0.4267,  0.4673,  0.3810,  ..., -0.4062, -0.5576,  0.1273],\n",
      "         [-0.0188,  0.0601, -1.7020,  ...,  0.1367, -0.3660,  0.3120],\n",
      "         [-0.5818,  0.4214, -0.3983,  ..., -1.2288,  0.1925, -0.7118],\n",
      "         ...,\n",
      "         [-0.4867, -0.0138, -1.2076,  ..., -0.9853,  0.0091, -0.5388],\n",
      "         [-0.8872, -0.2808, -1.0706,  ...,  0.4723,  0.5479, -0.0647],\n",
      "         [ 0.0023, -0.4293, -1.1973,  ...,  0.3119,  0.8496, -0.3050]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask4**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "False\n",
      "*********************self_output**********************\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.3833,  0.1955, -0.1124,  ...,  0.2242,  0.2528, -0.9373],\n",
      "        [-0.3833,  0.1955, -0.1124,  ...,  0.2242,  0.2528, -0.9373],\n",
      "        [-0.3833,  0.1955, -0.1124,  ...,  0.2242,  0.2528, -0.9373],\n",
      "        ...,\n",
      "        [-0.3833,  0.1955, -0.1124,  ...,  0.2242,  0.2528, -0.9373],\n",
      "        [-0.3833,  0.1955, -0.1124,  ...,  0.2242,  0.2528, -0.9373],\n",
      "        [-0.3833,  0.1955, -0.1124,  ...,  0.2242,  0.2528, -0.9373]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer4_output**********************\n",
      "tensor([[[-0.7636,  0.6965,  0.2417,  ..., -0.1499, -0.3011, -0.6514],\n",
      "         [-0.3076,  0.2771, -1.8334,  ...,  0.2922, -0.0538, -0.5088],\n",
      "         [-0.3316,  0.7795, -0.6973,  ..., -0.7909,  0.7672, -2.4855],\n",
      "         ...,\n",
      "         [-0.1614,  0.3732, -1.3530,  ..., -0.6902,  0.4410, -1.9257],\n",
      "         [-0.6918,  0.3565, -1.3136,  ...,  0.4989,  1.2365, -1.7010],\n",
      "         [ 0.3876,  0.2691, -1.3067,  ...,  0.2052,  1.6186, -1.5997]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask5**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "False\n",
      "*********************self_output**********************\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.3565, -0.2461, -0.2654,  ...,  0.3192, -0.0697, -0.5338],\n",
      "        [-0.3565, -0.2461, -0.2654,  ...,  0.3192, -0.0697, -0.5338],\n",
      "        [-0.3565, -0.2461, -0.2654,  ...,  0.3192, -0.0697, -0.5338],\n",
      "        ...,\n",
      "        [-0.3565, -0.2461, -0.2654,  ...,  0.3192, -0.0697, -0.5338],\n",
      "        [-0.3565, -0.2461, -0.2654,  ...,  0.3192, -0.0697, -0.5338],\n",
      "        [-0.3565, -0.2461, -0.2654,  ...,  0.3192, -0.0697, -0.5338]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer5_output**********************\n",
      "tensor([[[-0.9572,  0.3750, -0.1430,  ..., -0.0774, -0.2520, -0.6877],\n",
      "         [-0.4859, -0.1246, -2.1102,  ...,  0.3449, -0.0068, -0.5918],\n",
      "         [ 0.2414,  0.6377, -1.2024,  ...,  0.6696,  0.9577, -2.5586],\n",
      "         ...,\n",
      "         [-0.2181,  0.6533, -1.9105,  ...,  0.6167,  0.5015, -2.1910],\n",
      "         [-0.5864,  0.5151, -2.1713,  ...,  1.8464,  1.2291, -1.8108],\n",
      "         [ 0.6482,  0.4759, -1.9530,  ...,  1.3591,  1.4741, -1.6419]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask6**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "False\n",
      "*********************self_output**********************\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.1879, -0.1768, -0.0462,  ...,  0.0209, -0.1582, -0.2278],\n",
      "        [-0.1879, -0.1768, -0.0462,  ...,  0.0209, -0.1582, -0.2278],\n",
      "        [-0.1879, -0.1768, -0.0462,  ...,  0.0209, -0.1582, -0.2278],\n",
      "        ...,\n",
      "        [-0.1879, -0.1768, -0.0462,  ...,  0.0209, -0.1582, -0.2278],\n",
      "        [-0.1879, -0.1768, -0.0462,  ...,  0.0209, -0.1582, -0.2278],\n",
      "        [-0.1879, -0.1768, -0.0462,  ...,  0.0209, -0.1582, -0.2278]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer6_output**********************\n",
      "tensor([[[-0.9817,  0.1441, -0.3386,  ..., -0.1969, -0.3099, -0.6543],\n",
      "         [-0.6918, -0.4052, -2.2017,  ...,  0.2415,  0.0692, -0.7839],\n",
      "         [ 0.4331,  0.3004, -1.2241,  ...,  0.6510,  0.7861, -2.4296],\n",
      "         ...,\n",
      "         [ 0.1904,  0.0958, -2.0642,  ...,  0.2422, -0.0545, -1.8288],\n",
      "         [-0.1830,  0.0793, -2.1076,  ...,  1.4353,  0.7862, -1.7436],\n",
      "         [ 1.2658, -0.1573, -1.8448,  ...,  0.8187,  1.1478, -1.1992]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask7**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "False\n",
      "*********************self_output**********************\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[-0.0194, -0.1941,  0.0406,  ..., -0.0827, -0.0940,  0.0470],\n",
      "        [-0.0194, -0.1941,  0.0406,  ..., -0.0827, -0.0940,  0.0470],\n",
      "        [-0.0194, -0.1941,  0.0406,  ..., -0.0827, -0.0940,  0.0470],\n",
      "        ...,\n",
      "        [-0.0194, -0.1941,  0.0406,  ..., -0.0827, -0.0940,  0.0470],\n",
      "        [-0.0194, -0.1941,  0.0406,  ..., -0.0827, -0.0940,  0.0470],\n",
      "        [-0.0194, -0.1941,  0.0406,  ..., -0.0827, -0.0940,  0.0470]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer7_output**********************\n",
      "tensor([[[-1.0549,  0.1078, -0.3945,  ..., -0.2317, -0.3753, -0.4906],\n",
      "         [-0.7443, -0.6453, -2.1264,  ...,  0.1922,  0.0346, -0.5748],\n",
      "         [ 0.0545,  0.8124, -1.1405,  ...,  1.8666,  1.1353, -1.6852],\n",
      "         ...,\n",
      "         [-0.1283,  0.6716, -2.1061,  ...,  1.0020, -0.3472, -0.9127],\n",
      "         [-0.4886,  0.6826, -2.3897,  ...,  2.2069,  0.3287, -0.5875],\n",
      "         [ 1.0339,  0.1726, -2.1390,  ...,  0.8505,  0.9839, -0.3035]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask8**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "False\n",
      "*********************self_output**********************\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[ 0.0264,  0.0163,  0.2811,  ..., -0.1230, -0.2639,  0.1590],\n",
      "        [ 0.0264,  0.0163,  0.2811,  ..., -0.1230, -0.2639,  0.1590],\n",
      "        [ 0.0264,  0.0163,  0.2811,  ..., -0.1230, -0.2639,  0.1590],\n",
      "        ...,\n",
      "        [ 0.0264,  0.0163,  0.2811,  ..., -0.1230, -0.2639,  0.1590],\n",
      "        [ 0.0264,  0.0163,  0.2811,  ..., -0.1230, -0.2639,  0.1590],\n",
      "        [ 0.0264,  0.0163,  0.2811,  ..., -0.1230, -0.2639,  0.1590]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer8_output**********************\n",
      "tensor([[[-1.2887,  0.1283, -0.2947,  ..., -0.0804, -0.9325, -0.5273],\n",
      "         [-0.9634, -0.5895, -2.0890,  ...,  0.3170, -0.4592, -0.6741],\n",
      "         [-0.0395,  0.2662, -1.5647,  ...,  2.1216,  0.4955, -1.8580],\n",
      "         ...,\n",
      "         [-0.2235,  0.4189, -2.1917,  ...,  1.2633, -0.8740, -1.0245],\n",
      "         [-0.6371,  0.3945, -2.4796,  ...,  2.1997, -0.1940, -0.7715],\n",
      "         [ 1.0118, -0.4164, -2.4964,  ...,  1.3699,  0.1929, -0.1211]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask9**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "False\n",
      "*********************self_output**********************\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[0.0446, 0.1540, 0.7856,  ..., 0.1155, 0.1514, 0.3991],\n",
      "        [0.0446, 0.1540, 0.7856,  ..., 0.1155, 0.1514, 0.3991],\n",
      "        [0.0446, 0.1540, 0.7856,  ..., 0.1155, 0.1514, 0.3991],\n",
      "        ...,\n",
      "        [0.0446, 0.1540, 0.7856,  ..., 0.1155, 0.1514, 0.3991],\n",
      "        [0.0446, 0.1540, 0.7856,  ..., 0.1155, 0.1514, 0.3991],\n",
      "        [0.0446, 0.1540, 0.7856,  ..., 0.1155, 0.1514, 0.3991]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer9_output**********************\n",
      "tensor([[[-1.3733,  0.1971,  0.4250,  ...,  0.1682, -1.1226, -0.2269],\n",
      "         [-0.9495, -0.4445, -1.5416,  ...,  0.4052, -0.7014, -0.3237],\n",
      "         [ 0.4947,  0.5774, -1.1097,  ...,  2.3958,  0.2664, -1.4757],\n",
      "         ...,\n",
      "         [ 0.0811,  0.5574, -1.7198,  ...,  1.7523, -1.2435, -0.5218],\n",
      "         [-0.3316,  0.6332, -2.0327,  ...,  2.6048, -0.4967, -0.4547],\n",
      "         [ 1.3230,  0.0336, -1.6838,  ...,  1.4559, -0.5014,  0.2564]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask10**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "False\n",
      "*********************self_output**********************\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[ 0.1936,  0.6703,  0.3370,  ..., -0.1204,  0.2475,  0.5443],\n",
      "        [ 0.1936,  0.6703,  0.3370,  ..., -0.1204,  0.2475,  0.5443],\n",
      "        [ 0.1936,  0.6703,  0.3370,  ..., -0.1204,  0.2475,  0.5443],\n",
      "        ...,\n",
      "        [ 0.1936,  0.6703,  0.3370,  ..., -0.1204,  0.2475,  0.5443],\n",
      "        [ 0.1936,  0.6703,  0.3370,  ..., -0.1204,  0.2475,  0.5443],\n",
      "        [ 0.1936,  0.6703,  0.3370,  ..., -0.1204,  0.2475,  0.5443]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer10_output**********************\n",
      "tensor([[[-1.2204,  0.6304,  0.9796,  ..., -0.1501, -0.4586,  0.3854],\n",
      "         [-0.6353,  0.6857, -1.1121,  ...,  0.0107, -0.4440,  0.3066],\n",
      "         [ 0.7299,  1.4892, -0.4661,  ...,  2.2574,  1.0241, -1.1411],\n",
      "         ...,\n",
      "         [ 0.4244,  1.5300, -1.2657,  ...,  1.8226, -0.7139, -0.1537],\n",
      "         [-0.0814,  1.5125, -1.5273,  ...,  2.5419, -0.0352, -0.0832],\n",
      "         [ 1.3620,  1.2308, -1.2715,  ...,  1.3048, -0.2643,  0.2887]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************head_mask11**********************\n",
      "None\n",
      "*********************output_attentions**********************\n",
      "False\n",
      "*********************head_mask**********************\n",
      "False\n",
      "*********************self_output**********************\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], grad_fn=<ViewBackward0>)\n",
      "*********************attention_output**********************\n",
      "tensor([[0.3152, 0.2694, 0.4254,  ..., 0.2627, 0.0211, 0.1253],\n",
      "        [0.3152, 0.2694, 0.4254,  ..., 0.2627, 0.0211, 0.1253],\n",
      "        [0.3152, 0.2694, 0.4254,  ..., 0.2627, 0.0211, 0.1253],\n",
      "        ...,\n",
      "        [0.3152, 0.2694, 0.4254,  ..., 0.2627, 0.0211, 0.1253],\n",
      "        [0.3152, 0.2694, 0.4254,  ..., 0.2627, 0.0211, 0.1253],\n",
      "        [0.3152, 0.2694, 0.4254,  ..., 0.2627, 0.0211, 0.1253]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "*********************encoder_layer11_output**********************\n",
      "tensor([[[-0.9138,  0.5332,  0.8449,  ..., -0.0299, -0.3513,  0.9507],\n",
      "         [-0.2528,  0.4854, -0.3277,  ..., -0.1222, -0.1907,  0.5919],\n",
      "         [ 1.0506,  1.6898,  0.2773,  ...,  2.0845,  1.3129, -0.8347],\n",
      "         ...,\n",
      "         [ 0.8269,  1.5241, -0.7749,  ...,  1.7707, -0.5802,  0.1514],\n",
      "         [ 0.4310,  1.4911, -0.8805,  ...,  2.4706,  0.1369,  0.1914],\n",
      "         [ 1.9031,  1.4895, -0.6251,  ...,  1.1126,  0.0064,  0.6486]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "*********************encoder_output**********************\n",
      "BaseModelOutput(last_hidden_state=tensor([[[-0.9138,  0.5332,  0.8449,  ..., -0.0299, -0.3513,  0.9507],\n",
      "         [-0.2528,  0.4854, -0.3277,  ..., -0.1222, -0.1907,  0.5919],\n",
      "         [ 1.0506,  1.6898,  0.2773,  ...,  2.0845,  1.3129, -0.8347],\n",
      "         ...,\n",
      "         [ 0.8269,  1.5241, -0.7749,  ...,  1.7707, -0.5802,  0.1514],\n",
      "         [ 0.4310,  1.4911, -0.8805,  ...,  2.4706,  0.1369,  0.1914],\n",
      "         [ 1.9031,  1.4895, -0.6251,  ...,  1.1126,  0.0064,  0.6486]]],\n",
      "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)\n",
      "BaseModelOutputWithPooling(last_hidden_state=tensor([[[-0.4883,  0.2070,  0.3518,  ..., -0.1706, -0.1482,  0.5147],\n",
      "         [-0.1035,  0.1358, -0.2293,  ..., -0.2114, -0.0495,  0.2527],\n",
      "         [ 0.5810,  0.7122,  0.0684,  ...,  0.9229,  0.7045, -0.3998],\n",
      "         ...,\n",
      "         [ 0.4795,  0.6717, -0.4896,  ...,  0.8077, -0.2626,  0.0979],\n",
      "         [ 0.2302,  0.6038, -0.5332,  ...,  1.0918,  0.0958,  0.0982],\n",
      "         [ 0.9961,  0.6028, -0.3925,  ...,  0.4052,  0.0404,  0.3366]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 3.2776e-01,  2.6616e-01, -2.1126e-01, -1.7425e-01,  2.7719e-02,\n",
      "          7.0740e-02,  2.2413e-01, -2.8251e-01,  1.2420e-01, -1.7173e-01,\n",
      "          4.3667e-02, -1.9949e-01, -9.4729e-02, -7.2362e-02, -2.7521e-01,\n",
      "         -3.3015e-01, -8.1321e-02, -3.5914e-01, -6.1387e-03,  1.2847e-01,\n",
      "          2.1342e-01,  1.4660e-01,  1.4755e-02, -2.2448e-01, -1.1252e-01,\n",
      "         -1.3451e-01,  4.5886e-02, -2.7188e-01, -2.2057e-01,  1.9838e-01,\n",
      "         -1.2064e-01, -1.2183e-01,  3.6891e-01,  1.6030e-01, -5.6851e-02,\n",
      "          7.0515e-02, -1.7772e-01, -1.2475e-01, -2.0291e-01,  1.0008e-01,\n",
      "         -7.4499e-02, -2.3966e-01, -1.3690e-01, -1.4648e-01,  1.3115e-01,\n",
      "         -1.5939e-01,  3.2800e-01, -6.5078e-02, -1.7131e-01, -4.4175e-02,\n",
      "          3.2332e-01, -1.7415e-01,  1.6911e-01,  4.7964e-02,  1.1971e-01,\n",
      "          2.3617e-01, -9.1106e-02,  8.0420e-03,  2.2955e-01,  3.1252e-01,\n",
      "         -3.6416e-01,  6.0967e-02, -5.0724e-02,  1.3888e-02,  3.4240e-01,\n",
      "          3.3894e-02, -1.7499e-01,  2.1285e-01, -1.0390e-03, -1.2523e-01,\n",
      "         -1.6480e-01,  4.7665e-02,  7.5438e-02,  1.8905e-01, -8.8845e-02,\n",
      "         -2.6732e-01,  6.3309e-02, -3.1900e-02, -1.1654e-01, -1.5918e-02,\n",
      "         -1.7136e-01, -3.6575e-01,  2.6828e-01, -5.9398e-01, -3.0229e-02,\n",
      "         -3.5014e-01, -8.5088e-02, -1.2856e-02,  6.6197e-02, -5.7260e-02,\n",
      "          1.8387e-01,  1.3647e-01, -8.9628e-03,  3.6566e-01, -2.9408e-01,\n",
      "         -5.8237e-02,  2.3545e-01, -1.5774e-01,  1.1226e-01, -1.7616e-02,\n",
      "          1.1010e-01,  9.1026e-02,  4.3595e-02, -2.5577e-01,  1.1058e-01,\n",
      "          1.7665e-01,  1.6925e-01,  2.3827e-01, -2.2137e-01, -9.6351e-02,\n",
      "          5.0717e-02,  3.9682e-02, -1.6081e-01,  8.2403e-02,  2.6216e-01,\n",
      "         -2.1967e-01,  1.0045e-01,  1.2611e-01,  1.4632e-01,  3.2750e-01,\n",
      "         -3.3905e-01,  5.6572e-02,  1.0137e-01, -2.0480e-01, -3.2693e-01,\n",
      "          9.7941e-02,  1.0430e-01,  1.4098e-01,  2.5184e-02, -7.6590e-03,\n",
      "          2.1957e-01, -1.7124e-01,  3.3341e-01,  1.1613e-02,  1.9376e-01,\n",
      "          1.2132e-01, -6.1231e-02,  3.3242e-01,  8.1562e-02, -2.9706e-02,\n",
      "          7.9754e-02, -2.0210e-01, -5.9082e-02, -3.9192e-01, -1.5949e-01,\n",
      "          5.7367e-02, -3.3496e-01, -4.8364e-01, -2.2456e-01, -1.0883e-02,\n",
      "          1.9818e-01, -1.8263e-01, -1.0337e-01, -1.5347e-01, -2.7530e-01,\n",
      "          1.1695e-01, -2.3007e-01,  1.1879e-01, -1.4929e-03, -2.5644e-01,\n",
      "          8.6867e-02, -7.7149e-02, -1.0904e-01,  4.0291e-01,  4.7182e-01,\n",
      "          6.3073e-02, -2.6807e-01,  3.8929e-01,  1.1721e-01, -2.3912e-01,\n",
      "          2.0791e-01, -3.3582e-01,  9.8066e-02,  8.7450e-02, -1.1309e-01,\n",
      "          3.0831e-01,  1.4596e-01,  1.2568e-01,  1.7216e-01, -5.1644e-02,\n",
      "          7.1437e-02, -3.9528e-01, -3.5776e-01,  6.2227e-03,  2.9993e-01,\n",
      "         -2.8369e-01,  2.0457e-01,  1.5983e-01,  2.6941e-01,  3.3409e-01,\n",
      "          4.2313e-01, -2.0823e-01, -1.0190e-01, -7.2364e-02, -1.7144e-02,\n",
      "         -2.4407e-02,  1.7042e-02, -5.3189e-02, -1.2522e-01, -3.9730e-01,\n",
      "          7.4927e-02, -1.2082e-01,  1.1891e-01,  4.0024e-01, -3.5927e-01,\n",
      "         -2.0440e-01,  3.7576e-02, -2.0672e-01,  3.2538e-02, -6.1547e-02,\n",
      "         -1.3047e-01, -1.9031e-02,  1.5672e-01,  1.6726e-02,  1.8060e-01,\n",
      "         -1.5473e-01, -5.4084e-02,  1.8159e-01,  3.5343e-01,  3.0767e-01,\n",
      "          3.0097e-01,  5.6027e-02,  2.3579e-02, -4.1107e-02, -1.9810e-01,\n",
      "          8.5809e-02,  1.6858e-01, -1.3683e-01, -1.9587e-01,  3.0592e-01,\n",
      "         -1.7134e-01, -1.9937e-02, -8.2197e-02, -2.0545e-01, -2.8588e-01,\n",
      "          2.0152e-01,  6.5395e-03,  7.0230e-02, -2.7735e-01,  1.3240e-01,\n",
      "          2.4646e-01,  3.0225e-01, -3.0673e-01, -1.5355e-01,  3.3504e-02,\n",
      "          1.3733e-01,  8.9748e-02,  1.6962e-01, -2.3306e-01,  1.7398e-01,\n",
      "         -2.9799e-01,  2.1100e-02,  2.0230e-01, -1.4352e-01,  1.5379e-01,\n",
      "          1.3585e-01, -5.8019e-02, -9.6058e-04, -7.8940e-02,  2.4134e-01,\n",
      "         -3.2787e-02,  2.4101e-01,  1.1589e-01,  2.3056e-01,  3.7119e-01,\n",
      "         -2.1379e-02, -4.1413e-01,  6.6747e-02, -7.8096e-02,  8.3961e-02,\n",
      "         -2.0782e-01,  8.8565e-02, -1.7537e-01,  3.7531e-01, -2.9045e-01,\n",
      "         -1.1466e-02,  2.7869e-02,  7.3714e-02,  3.0864e-01,  5.5879e-02,\n",
      "          2.0754e-02,  1.0428e-01,  1.5491e-01,  5.4691e-01, -1.3984e-01,\n",
      "          2.9329e-01,  8.6240e-02, -3.5396e-02,  9.1298e-02,  1.4662e-01,\n",
      "         -1.1979e-01, -4.3287e-02,  1.6266e-01,  1.4587e-01, -2.8377e-01,\n",
      "         -9.3472e-02, -5.2835e-02,  7.9903e-03, -8.3049e-02,  1.4674e-02,\n",
      "          1.8320e-01, -1.0736e-01, -4.4740e-01, -1.7660e-01,  1.7955e-01,\n",
      "         -1.0368e-02, -1.1465e-01,  6.0935e-04,  1.8994e-01,  3.8290e-01,\n",
      "         -2.4620e-01, -4.7084e-02, -3.2415e-01,  1.0584e-01, -1.8586e-02,\n",
      "          1.3939e-02,  8.4545e-02, -4.8206e-01,  2.4578e-01, -1.3412e-01,\n",
      "         -2.4050e-01, -2.7044e-02,  1.1704e-01,  3.1924e-01,  7.8679e-02,\n",
      "          1.3704e-01, -6.0970e-02,  1.3601e-01,  2.3915e-01, -4.2153e-01,\n",
      "          1.2462e-01,  6.8571e-02,  1.3096e-01,  1.4963e-01, -2.8844e-01,\n",
      "         -2.5874e-01, -4.3058e-01,  2.3457e-02, -1.4960e-01, -3.7285e-02,\n",
      "          7.3484e-02, -2.0124e-01, -1.5783e-01, -1.4351e-01,  1.9244e-01,\n",
      "         -2.7130e-02,  2.7465e-01,  1.2990e-01, -5.5396e-02, -3.8838e-02,\n",
      "         -2.1078e-01,  5.0740e-01,  9.3196e-02, -4.2394e-02,  1.4226e-01,\n",
      "         -1.6508e-01, -2.6689e-01, -3.3777e-01,  4.6105e-03,  1.4487e-01,\n",
      "          9.4216e-02,  1.1023e-01, -3.7611e-01,  1.4393e-01,  1.7575e-01,\n",
      "         -3.0272e-02, -5.8074e-02,  2.2691e-02, -3.5269e-02, -7.0161e-02,\n",
      "          3.8902e-01, -1.1846e-01,  4.5458e-02, -1.3260e-01,  5.7295e-01,\n",
      "         -1.5209e-01, -1.6153e-02,  4.3524e-01, -7.1595e-02,  1.8860e-01,\n",
      "          1.0862e-01, -3.1478e-01, -7.2135e-02,  3.9145e-02,  2.4692e-01,\n",
      "         -6.0503e-02, -4.1931e-01,  4.9083e-01, -1.6707e-01,  1.0186e-01,\n",
      "          1.0818e-02,  3.5416e-03,  1.2538e-01, -2.5164e-01,  3.7156e-01,\n",
      "          1.1853e-01, -2.8479e-01,  9.1302e-02, -9.5983e-02,  4.4733e-01,\n",
      "         -1.9637e-01,  1.6856e-01, -5.9516e-02, -1.2432e-01, -3.1763e-01,\n",
      "          7.1512e-02, -3.8425e-01,  2.5069e-01, -4.3608e-01,  3.3864e-01,\n",
      "          4.6915e-02,  3.0493e-02, -4.9719e-02,  2.0155e-01, -3.8550e-01,\n",
      "         -2.4011e-01,  1.8862e-01, -9.1690e-02,  1.3810e-01, -2.0846e-01,\n",
      "          3.0380e-01, -2.7266e-02, -1.0506e-01,  2.2766e-02,  1.1617e-01,\n",
      "         -4.5467e-02, -5.5085e-03,  2.3337e-01,  1.8352e-01, -6.5456e-02,\n",
      "         -2.1161e-01,  1.5526e-01,  1.7099e-01,  4.5511e-02, -2.5698e-02,\n",
      "          2.7652e-02, -2.3633e-01,  2.5005e-01,  9.6686e-02,  1.6625e-01,\n",
      "         -2.4125e-01, -2.8725e-01,  2.7362e-03,  8.7923e-02, -2.9609e-01,\n",
      "         -1.3540e-01,  2.1506e-02, -7.5425e-02, -2.1552e-03,  1.0838e-01,\n",
      "         -2.0043e-01,  8.9697e-02,  1.7441e-01,  1.6636e-01,  3.2477e-01,\n",
      "         -1.0331e-01,  7.8751e-02, -1.7097e-01,  7.9673e-02,  2.4269e-01,\n",
      "          3.0216e-01, -3.7283e-01,  1.3204e-01,  8.9231e-03, -9.6231e-02,\n",
      "          2.3521e-02,  1.3739e-01,  1.4939e-01,  3.9374e-01, -1.8906e-01,\n",
      "          1.7177e-01,  8.9476e-04, -2.7315e-01, -7.7855e-02, -9.4117e-02,\n",
      "          4.0569e-02, -1.6779e-03,  3.9175e-02,  4.2938e-02, -7.2388e-02,\n",
      "         -3.8663e-02, -4.1721e-01, -2.4363e-02,  5.0959e-02,  2.9851e-01,\n",
      "          4.0705e-02,  3.6003e-01,  9.2501e-02,  1.1166e-01, -3.4972e-02,\n",
      "         -2.0125e-01, -8.6287e-02, -1.9209e-01,  2.2818e-01,  3.4347e-01,\n",
      "         -1.2145e-01,  1.0993e-01,  2.7966e-02, -5.3702e-03,  1.2106e-01,\n",
      "          1.3625e-01, -3.3133e-01,  3.4292e-01, -1.4867e-01, -3.6053e-02,\n",
      "          5.2621e-02, -2.8926e-01,  2.0735e-01,  5.2321e-02,  1.3856e-01,\n",
      "         -6.5832e-02, -2.6111e-01, -2.3080e-02,  5.6368e-02,  3.1002e-01,\n",
      "         -1.0687e-01, -6.0332e-02, -2.2603e-01, -1.8166e-01,  3.9190e-01,\n",
      "          1.3308e-01,  2.9048e-01, -1.8214e-01, -3.1446e-01,  2.4503e-01,\n",
      "         -2.2504e-01,  2.1325e-01,  2.1137e-01,  2.6444e-01, -6.8670e-02,\n",
      "         -3.9511e-02,  3.3274e-02,  5.4856e-02,  6.4055e-02, -3.4543e-01,\n",
      "         -2.8740e-01,  6.8110e-02, -2.1741e-01, -1.0956e-03,  3.2977e-01,\n",
      "         -3.7598e-02,  1.2231e-02,  2.2179e-01,  1.4994e-02,  1.6189e-01,\n",
      "          1.0582e-01, -1.4986e-01,  7.0170e-02,  8.5326e-02, -5.3171e-02,\n",
      "         -5.9358e-02,  6.5444e-02,  4.8644e-02, -3.0258e-02,  1.2361e-01,\n",
      "         -2.2695e-01,  3.2714e-01,  1.9953e-01, -5.6319e-02, -1.4121e-01,\n",
      "          1.7131e-01, -6.9846e-03, -4.4249e-01, -2.2401e-01,  2.8764e-01,\n",
      "          1.3088e-01,  2.5875e-01, -2.9326e-01,  1.6724e-01, -2.2827e-01,\n",
      "         -1.9200e-01,  1.2168e-01,  1.8101e-01, -6.9949e-02, -3.5771e-01,\n",
      "          4.7984e-02,  2.6294e-01,  1.6977e-01, -3.5377e-02, -2.0972e-03,\n",
      "          2.3085e-01, -1.8749e-01,  4.8446e-02,  3.8408e-01,  1.4171e-01,\n",
      "          4.3082e-03, -2.0774e-01,  2.2515e-01, -8.3740e-02,  8.4926e-02,\n",
      "         -1.2453e-01, -5.7579e-02,  3.9041e-01,  9.4083e-02, -1.7502e-01,\n",
      "         -9.8023e-02,  1.0634e-01,  3.7319e-04,  3.8829e-02,  3.3523e-01,\n",
      "          2.9110e-01,  1.9310e-01, -1.0196e-01, -2.2372e-01,  3.0160e-01,\n",
      "         -9.3562e-02,  1.6739e-01,  1.0322e-01, -6.4055e-02, -1.0595e-02,\n",
      "         -5.4651e-01, -3.5369e-01, -1.3519e-01, -1.1673e-01, -6.0839e-02,\n",
      "          4.3693e-01, -1.5412e-01, -8.9598e-02, -2.0958e-01, -1.2384e-01,\n",
      "         -1.7667e-01, -6.1389e-02,  4.1208e-01,  8.4924e-02, -4.2293e-02,\n",
      "         -2.2382e-01, -1.0021e-01, -2.0940e-02,  4.9752e-02,  3.3286e-01,\n",
      "         -1.9598e-01,  1.3039e-01, -1.4670e-01, -1.5181e-01,  2.3574e-01,\n",
      "         -1.2580e-01,  2.1278e-01, -3.0812e-01,  2.0746e-01,  1.0852e-01,\n",
      "         -1.0385e-01,  2.3931e-01,  3.8094e-01,  3.3093e-02, -1.0574e-01,\n",
      "         -3.0840e-02,  1.9628e-01, -3.3007e-01,  3.2999e-02, -8.5853e-02,\n",
      "          1.3077e-01, -2.3245e-01,  1.2127e-01,  1.5345e-01,  3.0036e-01,\n",
      "         -2.7930e-01, -1.9432e-01, -9.1728e-02, -5.9147e-02,  7.4909e-02,\n",
      "         -1.5097e-01, -3.1080e-02,  7.2784e-02,  1.6065e-01, -2.1248e-02,\n",
      "          6.6453e-02, -1.9228e-01,  5.2412e-02,  3.2254e-01, -3.4701e-03,\n",
      "          2.1451e-01, -1.8473e-01, -1.9165e-01,  8.4150e-03,  1.9475e-01,\n",
      "          2.4109e-01, -1.9947e-02, -2.9440e-01, -2.8899e-01, -1.3430e-01,\n",
      "         -4.5763e-01, -3.3973e-01, -1.4537e-01,  3.6540e-01,  2.3801e-01,\n",
      "         -1.8689e-02, -3.0637e-02, -1.1643e-01,  6.8456e-02,  8.7857e-02,\n",
      "          3.4626e-01, -1.5946e-02,  2.0764e-01, -3.2258e-01,  1.9246e-01,\n",
      "         -1.1467e-01, -2.5814e-02,  1.6677e-02, -7.0184e-03,  4.4499e-03,\n",
      "          4.1069e-01,  2.2116e-01,  2.3237e-01,  3.4158e-01,  1.0937e-01,\n",
      "          1.0316e-01, -1.2467e-01,  1.7851e-01,  5.6816e-02, -2.0241e-01,\n",
      "          2.4652e-01,  4.0906e-01,  2.5844e-01, -1.0922e-01, -1.2873e-01,\n",
      "          1.5531e-01,  1.4895e-01,  5.7897e-01,  2.5331e-02,  3.0714e-01,\n",
      "          2.8781e-01,  4.7829e-01,  3.0448e-01,  4.8597e-03,  1.1581e-01,\n",
      "          1.4758e-01, -2.0098e-02, -9.3780e-03,  2.2512e-02,  3.0821e-02,\n",
      "          8.1034e-02, -1.3029e-01, -3.4754e-01, -1.5723e-01, -7.1711e-02,\n",
      "         -2.3700e-01,  1.7622e-01,  1.6426e-01, -2.7761e-01, -9.9433e-02,\n",
      "          3.2916e-01, -5.7703e-01, -7.0749e-02,  2.8561e-01,  9.0121e-02,\n",
      "         -1.8582e-01, -2.0184e-01,  1.3028e-01,  4.3325e-01,  2.0480e-01,\n",
      "          2.4461e-01, -3.4143e-01,  1.4126e-01,  1.5641e-01, -1.3090e-01,\n",
      "          2.8126e-01, -8.9488e-02, -8.3900e-02,  1.1720e-01,  1.6367e-01,\n",
      "         -4.6844e-02, -1.6158e-02, -2.6965e-01,  8.8016e-02,  3.5081e-01,\n",
      "          2.8500e-01, -4.1876e-02, -2.0495e-02]], grad_fn=<TanhBackward0>), hidden_states=None, attentions=None)\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model_md(input))\n",
    "print(\"end\")\n",
    "print(model(input))\n",
    "# print(torch.equal(model(input),model_md(input)))\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
